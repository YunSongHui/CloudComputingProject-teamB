import requests
import re
import lxml.html
import MySQLdb

conn = MySQLdb.connect(db='Crawler', user='cloud', passwd='1111', charset='utf8mb4')

c=conn.cursor()

def crawling(page_count):
    front_url="http://www.jobkorea.co.kr/Starter/?JoinPossible_Stat=0&schOrderBy=0&LinkGubun=0&LinkNo=0&schType=0&schGid=0&Page="

    result=[]

    for i in range(1, page_count+1):
        url = front_url+str(i)

        temp_result=requests.get(url)
        root=lxml.html.fromstring(temp_result.content)
        for everything in root.cssselect('.filterList'):
            for thing in everything.cssselect('li'):
            
                company = thing.cssselect('.co .coTit a')
                result.append(company[0].text.strip())
                company = company[0].text.strip()

                t_url = thing.cssselect('.info .tit a')
                result.append(t_url[0].text_content().strip())
                title = t_url[0].text_content().strip()
                title_url = t_url[0].get('href')

                result.append(title_url)

                site_name = '잡코리아'
                result.append(site_name)

                field1 = thing.cssselect('.info .sTit span:nth-child(1)')
                result.append(field1[0].text)

                field2 = thing.cssselect('.info .sTit span:nth-child(2)')
                if not field2:
                    result.append('')
                elif field2:
                    result.append(field2[0].text)

                field3 = thing.cssselect('.info .sTit span:nth-child(3)')
                if not field3:
                    result.append('')
                elif field3:
                    result.append(field3[0].text)

                career = thing.cssselect('.sDesc strong')
                result.append(career[0].text)

                academic = thing.cssselect('.sDesc span:nth-child(2)')
                result.append(academic[0].text)

                area = thing.cssselect('.sDesc span:nth-child(3)')
                result.append(area[0].text)

                deadline = thing.cssselect('.side .day')
                result.append(deadline[0].text)

                insert_sql = 'INSERT INTO Recruitment_Info(company, title, titlelink, sitename, field1, field2, field3, career, academic, area, deadline) VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'

                insert_val = result[0], result[1], result[2], result[3], result[4], result[5], result[6], result[7], result[8], result[9], result[10]

                c.execute(insert_sql, insert_val)

                conn.commit()

    return result

def main():
    page_count = 4
    result=crawling(page_count)

    print(result)
    print()

    conn.close()
    
main()
