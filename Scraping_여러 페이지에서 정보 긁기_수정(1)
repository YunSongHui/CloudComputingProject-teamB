import requests
import re
import lxml.html
from bs4 import BeautifulSoup
import pandas

def crawling(page_count):
    front_url="http://www.jobkorea.co.kr/Starter/?JoinPossible_Stat=0&schOrderBy=0&LinkGubun=0&LinkNo=0&schType=0&schGid=0&Page="

    result=[]

    for i in range(1, page_count+1):
        url = front_url+str(i)

        temp_result=requests.get(url)
        root=lxml.html.fromstring(temp_result.content)
        for everything in root.cssselect('.filterList'):
            for thing in everything.cssselect('li'):
                t_url = thing.cssselect('.info .tit a')
                result.append(t_url[0].text_content().strip())
                t_url = t_url[0].get('href')
                result.append(t_url)

                company = thing.cssselect('.co .coTit a')
                result.append(company[0].text.strip())

                field1 = thing.cssselect('.info .sTit span:nth-child(1)')
                result.append(field1[0].text)

                field2 = thing.cssselect('.info .sTit span:nth-child(2)')
                if not field2:
                    result.append('')
                elif field2:
                    result.append(field2[0].text)

                field3 = thing.cssselect('.info .sTit span:nth-child(3)')
                if not field3:
                    result.append('')
                elif field3:
                    result.append(field3[0].text)

                career = thing.cssselect('.sDesc strong')
                result.append(career[0].text_content())

                academic = thing.cssselect('.sDesc span:nth-child(2)')
                result.append(academic[0].text)

                location = thing.cssselect('.sDesc span:nth-child(3)')
                result.append(location[0].text)

                deadline = thing.cssselect('.side .day')
                result.append(deadline[0].text)

    return result


def main():
    page_count = 4
    result=crawling(page_count)

    print(result)
    print()


main()
